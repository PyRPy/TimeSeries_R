---
title: "PM25_Trend"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r message=FALSE}
library(ggplot2)
library(dplyr)
library(lubridate)
library(magrittr)

```

# Read and inspect the data
```{r}
pm25 <- read.table("Data_TS/PRSA_data_2010.1.1-2014.12.31.csv", sep=",", skip = 0,  header = T)
head(pm25)
tail(pm25)
```
# Preliminary EDA
```{r}
# summary(pm25)
qplot(data = pm25, x=year, y=pm2.5)
```
# Data wrangling 
* need to combined 'year', 'month', 'day', and 'hour' to build a time or date column 
Ref : https://stackoverflow.com/questions/26334763/merge-three-different-columns-into-a-date-in-r
```{r}
pm25$date <- with(pm25, paste(year, month,day,  sep="-"))
pm25$time <- with(pm25, paste(date, hour , sep=" "))
# pm25$date <- paste(pm25$year, pm25$mon, pm25$day, sep="-") %>% ymd() %>% as.Date()
pm25$time <- with(pm25, ymd_h(time))
head(pm25)
```
Check the data structure and plot it again 
```{r}
str(pm25)
qplot(data = pm25, x=time, y=pm2.5) # messy plot
```
Plot a shorter time frame (before 2011)
```{r}
pm25 %>% filter(time < "2011-01-01") %>%
  ggplot(aes(x=time, y=pm2.5)) +
  geom_line()+
  ylab("pm2.5 mg/m3")+
  xlab("") + 
  ggtitle("PM2.5 Trend Over Time")
```
How many data points that are 'NA' and to sample daily average of the pm25 levels
```{r}
pm25_day <- pm25 %>% 
  na.omit() %>%
  group_by(Time= cut(time, 'days')) %>% 
  summarise(pm2.5=mean(pm2.5))

head(pm25_day)
# qplot(x=pm25_day$Time, y=pm25_day$pm2.5)
```
Plot the pm25 again, it shows the interval patterns over time
* after sampling the 'time' converted to 'factor', need to convert back to date, but first do the 'character' then 'date, it needs two steps.
```{r}
pm25_day %>% 
  ggplot(aes(x=as.Date(as.character(Time)), y=pm2.5)) +
  geom_line()+
  ylab("pm2.5 mg/m3")+
  xlab("") + 
  ggtitle("PM2.5 Trend Over Time")
# Still too many points in the plot
```


```{r}
# dim(pm25)
# pm25 <- na.omit(pm25)
# str(pm25)
# pm25$pm2.5 <- as.numeric(pm25$pm2.5)
# pm25$TEMP <- as.numeric(pm25$TEMP)
pm25_trend <- pm25[, c("time", "pm2.5")]
```
# Data Preparation for Time Series analysis
Convert data set time series
```{r}
library(xts)
pm25_xts<-xts(pm25_trend[,-1], order.by = pm25_trend$time)
# looks like xts convert everything into 'char'
str(pm25_xts)
```
Plot the time series after conversion :
```{r}
# autoplot(pm25_xts[, "pm2.5"])
# plot.ts(pm25_xts)
# autoplot(pm25_xts)
head(pm25_xts)
```
To smooth out the pm2.5 concentration - weekly or 168 hour interval
```{r}
# add simple moving average
library("TTR")
# pm25_xts <- na.omit(pm25_xts)
# head(pm25_xts)
pm25_xts <- na.omit(pm25_xts)
# pm25_xts$pm2.5 <- as.numeric(pm25_xts$pm2.5)
plot(SMA(pm25_xts, n=168)) # something not right about this trend
```
Plot it on a 'weekly' basis, which is 168 hours equivalent
```{r}
# autoplot(pm25_xts)
pm25_xts_sma <- SMA(pm25_xts, 168) # 24 hour average, or per day
autoplot(pm25_xts_sma)
```
```{r}
# pm25_day$Time <- ymd_hms(pm25_day$Time)
# pm25_day <- na.omit(pm25_day)
# head(pm25_day)
# pm25_day
# pm25_xts_day<-xts(pm25_day[,-1], order.by = pm25_day$Time)
# head(pm25_xts_day)
# autoplot(pm25_xts_day)
```
Make 'Difference' on data to make data stationary on mean (remove trend)
```{r}
autoplot(diff(pm25_xts),ylab='Differenced CO concentration')
```
It looks like there are still some lumps around the 'year' transition period

Plot ACF and PACF to identify potential AR and MA model
```{r fig.height=3, fig.width=8}
par(mfrow = c(1,2))
acf(na.omit(diff(pm25_xts)),main='pm25')
pacf(na.omit(diff(pm25_xts)),main='pm25')
```
Identification of best fit ARIMA model
```{r}
require(forecast)
ARIMAfit <- auto.arima(pm25_xts, approximation=FALSE,trace=FALSE)
summary(ARIMAfit)
```
Forecast CO using the best fit ARIMA model 
https://stackoverflow.com/questions/7154133/plot-new-has-not-been-called-yet
trouble shooting - run the lines together, select all lines and run ! solved
```{r}
pred.steps <- 48 # hours
par(mfrow = c(1,1))
pred <- predict(ARIMAfit, n.ahead = pred.steps)
# pred
# plot(pm25_xts,type='l', xlab = 'Hour', ylab = 'pm2.5 levels')
# lines(pred$pred,col='blue')
# lines(pred$pred+2*pred$se,col='orange')
# lines(pred$pred-2*pred$se,col='orange')

plot(x=c(1:pred.steps), y=pred$pred,col='blue')
```
Observe the ACF :
```{r message=FALSE, warning=FALSE}
library(fpp2)
ggAcf(pm25_xts) +
  ggtitle("pm2.5 concentration ACF profile")
```
Use ARIMA model to forecast the CO concentration :
```{r}
fit_arima<-auto.arima(pm25_xts)
summary(fit_arima)
```
To preidct the next hour CO concentration :
```{r}
fit_arima%>%forecast(h=72)%>%autoplot()
```
The Arima model is not that 'impressive', let's try ets model for the forecasting :
```{r}
pm25_xts%>%ets()%>% forecast(h=168) %>%autoplot()
```
```{r}
checkresiduals(fit_arima)
```
# Data Preparation for the LSTM model from KerasR
Transform data into matrix format required for nueral network, first remove the time column
If it is only one column in the data frame after removing date column, you don't need to transform by 'matrix'
```{r}
data <- pm25_trend$pm2.5
# length(data)
# dim(data)
# head(pm25_trend)
# need to remove 'NA's
data <- na.omit(data)
dim(data)
```
To normalize the data by Z-score method
```{r}
train_data <- data[1:25000]
mean <- mean(train_data)
std <- sd(train_data)
data <- scale(data, center = mean, scale = std)
```

Reference : Deep Learning with R, 2018
```{r}
library(keras)
generator <- function(data, lookback, delay, min_index, max_index,
                      shuffle = FALSE, batch_size = 128, step = 6) {
  if (is.null(max_index))
    max_index <- nrow(data) - delay - 1
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size, max_index))
      i <<- i + length(rows)
    }
    
    samples <- array(0, dim = c(length(rows), 
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
                     
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]], 
                     length.out = dim(samples)[[2]])
      samples[j,,] <- data[indices,]
      targets[[j]] <- data[rows[[j]] + delay,1]
    }            
    
    list(samples, targets)
  }
}
```
## Define the time steps, train, validation and test time steps
```{r}
lookback <-720 # one year or 8760 hours 365 x 24= 8760
step <- 24
delay <- 168 # a week delay
batch_size <- 128 # work hours

train_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 25000,
  shuffle = FALSE,
  step = step, 
  batch_size = batch_size
)

val_gen = generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 25001,
  max_index = 30000,
  step = step,
  batch_size = batch_size
)

test_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 30001,
  max_index = NULL,
  step = step,
  batch_size = batch_size
)

# This is how many steps to draw from `val_gen`
# in order to see the whole validation set:
val_steps <- (30000 - 25001 - lookback) / batch_size

# This is how many steps to draw from `test_gen`
# in order to see the whole test set:
test_steps <- (nrow(data) - 30000 - lookback) / batch_size
```
## Recurrent baseline
```{r}
model <- keras_model_sequential() %>% 
  layer_gru(units = 32, dropout=0.3, recurrent_dropout=0.3,
            input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)


```
## start iterations
```{r}
history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 10,
  validation_data = val_gen,
  validation_steps = val_steps
)
```

## plot errors trend
```{r}
plot(history)
```
## Make predictions based on the previous data and plot the preds
```{r}
prediction_test<-c()
for (i in 1: test_steps) {
    prediction.set <- test_gen()[[1]]  
    prediction <- predict(model, prediction.set)  

    prediction_test<-rbind(prediction_test, prediction)  
      
}

prediction_test_1 <- mean[[1]] + std[[1]]*prediction_test[, 1] # get back to original 
# prediction_real <- mean[[1]] + std[[1]]*prediction.set # get back to original 
plot(prediction_test_1, pch = 16, cex = .2, xlab = "Time-Hours", ylab = "pm2.5 levels")
# lines(prediction_real, col="red") # it's 'future' , no real data available
```
```{r}
qplot(y=prediction_test_1) +
  ylab("pm2.5 levels")
```


